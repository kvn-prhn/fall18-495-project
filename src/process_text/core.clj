(ns process-text.core
  (:gen-class)
  (:require [net.cgrand.enlive-html :as html])
  (:require [org.httpkit.client :as httpkit])
  (:require [clojure.pprint :as pprint])
  (:require [clj-http.client :as client])
  (:require [cheshire.core :as cheshire])
  )

(defn fetch-url [url]
	(html/html-resource (java.net.URL. url)))  ;TODO have a case for handling exceptions.

; NOT DONE YET
(defn load-word-list [f]
	"Load a file and return all of the words as a sequence."
	(with-open [rdr (clojure.java.io/reader f)]
		(clojure.string/join "\n" (distinct (line-seq rdr)))  
	))

(defn remove-punc 
	"Remove punctuation from the given string."
	[x] 
	(clojure.string/trim-newline 
		(clojure.string/replace
			(clojure.string/replace 
				(clojure.string/trim-newline x) 
				#"[\"\.\\,=+%()\$!?<>;']" 
				" ")
			"\n" ; second replace
			"")))

(defn extract-wiki-text 
	"given a wikipedia article in enlive form, get only the article text."
	[text]
	(defn iter-down
		[t] 
		(if (seq? t)
			(concat (map iter-down t))
			(if (nil? (get t :content))
				t
				(iter-down (get t :content))
			)
		)
	)
	(filter (fn [n] 
				(and
				(not (clojure.string/blank? n))
				(not (nil? (re-matches #"[0-9a-zA-Z]*" n)))
				(not (nil? (re-matches #"[a-zA-Z]*" n)))
			))
		(map clojure.string/lower-case (map clojure.string/trim (clojure.string/split 
			(remove-punc (clojure.string/join " " (flatten (iter-down 
				(flatten (html/select text [:div.mw-body-content :div.mw-parser-output :p])) 
			)))) ; this is the article
			#" "
	)))))
	
	
(defn wcount 
	"count the number of occurances of each word. Returns a dictionary."
	[wlist]
	(let [mcount_input {}]
	  (loop [i 0 mcount mcount_input]
		(if (>= i (count wlist))
		  mcount
		  (recur (inc i) (if (nil? (get mcount (nth wlist i)))
			(assoc mcount (nth wlist i) 1)
			(update mcount (nth wlist i) inc)
		  ))
		)
	))
)

(defn wcount-merge
	"given a list of word count maps, merge them all together and sum counts"
	[wlists]
	(println wlists)
	(reduce 
		(fn [wlist1 wlist2] 
			(loop [result wlist1 q wlist2]
				(if (= 0 (count q))
					result
					(let [ nextkey (first (keys q)) rmq (dissoc q nextkey)]
						(if (nil? (get result nextkey))
							(recur (assoc result nextkey 
								(get q nextkey)) rmq )
							(recur (assoc result nextkey 
								(+ (get result nextkey) (get q nextkey))) rmq )
						)
					)
				)
			)
		)
	wlists)
)

(defn wcount-normalize
	"given a word count map, return the same word count map but normalized."
	[wcount]
	(let [words (keys wcount) superCountSum 
		(loop [k 0 resultSum 0]
		  (if (>= k (count wcount))
		    resultSum
			(recur (inc k) (+ resultSum 
			  (get wcount (nth words k))
			))
		  )
		)
	  ]
	  (loop [k 0 resD {}]
	    (if (>= k (count (keys wcount)))
		  resD
		  (recur (inc k) (assoc resD (nth (keys wcount) k)
		    (/ (get wcount (nth (keys wcount) k)) superCountSum)
		  ))
		)
	  ) ; TODO
	)
)

(defn word-in-model 
	"get the chance that a word was generated using the given model m."
	[m w] ; model, word
	(if (contains? m w)
		(get m w)
		0))

(defn count-in-doc
	"get the count of a word w from a document d."
	[d w] ; model, word
	(if (contains? d w)
		(get d w)
		0))

		
(defn topic-probs
	[
		documentBows ; a list of all documents as bag-of-words (map string to count)
		vocabulary ; list of all words possible
		topicModelsInitial ; vector of models (map string to probability)
		bgModel ; since map (string to proability) for background words
		bgProb ; chance of using the background model.
	]
	(let ; what is being defined and constant? 
	  [
		doc-word-matrix ( ; matrix where rows = document, columns = word, value = count.
		  mapv (fn [docBow]
			(mapv 
			  (fn [word]
			    (count-in-doc docBow word)
			  )
			vocabulary)
		  ) documentBows)
		doc-word-count (fn [docIndex wordIndex] ; helper func for above matrix
			(nth (nth doc-word-matrix docIndex) wordIndex))
		topicModelIndices (range (count topicModelsInitial))
		documentBowsIndices (range (count documentBows))
		vocabularyIndices (range (count vocabulary))
	  ]
	(loop ; what is being defined to update at each step?
	  [
	    n 0 ; what iteration it is on.
		
		; initial topic models and probability of topics... will be updated in M-step.
		topicModels (mapv ;topicModels[topic][word] = chance of generating that word with topic.
		  (fn [topicIndex]
			(mapv 
			  (fn [wordIndex]
				(word-in-model (nth topicModelsInitial topicIndex) (nth vocabulary wordIndex))
			  )	(range (count vocabulary)) )
		  ) topicModelIndices)
			  
		piDocGenByTopic ( ; [document][topic] = probability doc was generated by topic
		  mapv (fn [docBowIndex]
		    (mapv (fn [topicIndex]
			  (/ 1 (count topicModels)) ; uniform distribution initially
			) topicModelIndices)
		  ) documentBowsIndices)
	  ]
	  (println "n =" n)
	  (println "piDocGenByTopic: " piDocGenByTopic)
	  ;(println "topicModels: " topicModels)
	  (let 
	    [ 
		; E-step hidden variables.
		zBgModelProbs ( ; [document][word] = probabiity it was generated by bg model.
		  mapv (fn [docBowIndex]
			(mapv 
			  (fn [wordIndex]
			    (let [word (nth vocabulary wordIndex)] 
				(if (contains? bgModel word)
			    (/ 
			      (* bgProb (word-in-model bgModel word))
				  (+
				    (* bgProb (word-in-model bgModel word))
				    (* 
					  (- 1 bgProb)
				      (reduce +  ; summation
					    (map (fn [topicIndex]
						  (*
						    (nth (nth piDocGenByTopic docBowIndex) topicIndex) ; probability of document generated by this topic
							(nth (nth topicModels topicIndex) wordIndex)
						  )
						) topicModelIndices)
					  )
				    )
				  )
				)
				0 ; if the word is not in the background model, then 0% of being generated.
				))
			  )
			vocabularyIndices)
		  ) documentBowsIndices)
		
		  zProbs ( ; [document][word][topic] = probabiity word in document was generated by topic.
		  mapv (fn [docBowIndex]
			(mapv 
			  (fn [wordIndex]
			    (let [word (nth vocabulary wordIndex)] 
			    ;(println "wordIndex =" wordIndex "word =" word)
			    (mapv 
				  (fn [topicIndex]
				    ;(println (nth topicModels topicIndex))
					(let [numer (reduce +  ; summation to normalize over all topics
					    (map (fn [topicIndexInner]
						  (*
							(nth (nth piDocGenByTopic docBowIndex) topicIndexInner) 
							(nth (nth topicModels topicIndexInner) wordIndex)
						  )
						) topicModelIndices)
						)]
				    (if (== numer 0)
					  0
					  (/ 
					    (* 
					      (nth (nth piDocGenByTopic docBowIndex) topicIndex) ; chance doc is generated by topic
						  (nth (nth topicModels topicIndex) wordIndex) ; chance of word being generated by topic
					    )
					    numer				  
					  )
					))
				  )
				topicModelIndices))
			  )
			vocabularyIndices)
		  ) documentBowsIndices)
	    ]
		;(pprint/pprint zBgModelProbs)
		;(pprint/pprint zProbs)
		(if (>= n 10)
		  piDocGenByTopic ; output when we're done.
		  (recur 
		    (inc n)
			
			; topicModels
		    (mapv ; [topic][word] = chance of generating that word with topic.
			  (fn [topicIndex]
			    (mapv 
				  (fn [wordIndex]
				  (let [denom (reduce + ; summation over documents
					    (mapv 
					      (fn [docBowIndex]
					  	    (*
						      (doc-word-count docBowIndex wordIndex)
							  (- 1 (nth (nth zBgModelProbs docBowIndex) wordIndex) )
							  (nth (nth (nth zProbs docBowIndex) wordIndex) topicIndex)
						    )
						  )
					    documentBowsIndices)) ]
				  (if (== denom 0)
				    0
				    (/
					  denom
					  (reduce + 
					    (mapv 
					      (fn [innerWordIndex] ; normalize over vocabulary
							(reduce + ; summation over documents
							  (mapv 
								(fn [docBowIndex]
								  (*
									(doc-word-count docBowIndex innerWordIndex)
									(- 1 (nth (nth zBgModelProbs docBowIndex) innerWordIndex) )
									(nth (nth (nth zProbs docBowIndex) innerWordIndex) topicIndex)
								  )
								)
							  documentBowsIndices)
							))
						vocabularyIndices )
					  )
				    )))
				  )	vocabularyIndices )
			  ) topicModelIndices) ; end topicModels
			
			; piDocGenByTopic
		    (mapv (fn [docBowIndex] ; [document][topic] = probability doc was generated by topic
				(mapv (fn [topicIndex]
				  (let [denom 
					  (reduce + 
					    (mapv 
						  (fn [wordIndex]
						    (*
							  (doc-word-count docBowIndex wordIndex)
							  (- 1 (nth (nth zBgModelProbs docBowIndex) wordIndex) )
							  (nth (nth (nth zProbs docBowIndex) wordIndex) topicIndex)
						    )
						  )
						  vocabularyIndices))
					] 
				    (if (== denom 0) 
					  0
					  (/
					    denom
						(reduce + 
						  (mapv 
						    (fn [outerTopicIndex]
						      (reduce + 
							  (mapv 
							    (fn [wordIndexInner]
								  (*
								    (doc-word-count docBowIndex wordIndexInner)
								    (- 1 (nth (nth zBgModelProbs docBowIndex) wordIndexInner) )
								    (nth (nth (nth zProbs docBowIndex) wordIndexInner) outerTopicIndex)
								  )
							    )
							    vocabularyIndices))
						    ) 
						  topicModelIndices))
					  )
					)
				  )
				) topicModelIndices)
			  ) documentBowsIndices) ; end piDocGenByTopic
		  ))
		) ; end inner let
	) ; end loop
  ) ; end first let
)



(defn wiki-url-to-bow 
	"Given a wikipedia title and language, convert the article to a Bag of Words with counts."
	[v] ;[title lang-short]
	(let [title (nth v 0) 
		  lang-short (nth v 1)	
		  url (clojure.string/join "" [ "https://" lang-short ".wikipedia.org/wiki/" title ])]
	(println (clojure.string/join "" ["Getting wikipedia article at " url]))
	(wcount (extract-wiki-text (fetch-url url)))))

(defn wiki-url-to-links-list
	"Given a wikipedia title and language, get the first 10 titles of articles that are links to the given article.."
	[v] ;[title lang-short]
	(let [title (nth v 0) 
	  lang-short (nth v 1)	
	  url (clojure.string/join "" [ "https://" lang-short ".wikipedia.org/w/api.php?action=query&prop=links&format=json&titles=" title ])]
		(let [jres (cheshire/parse-string (get (client/get url {:cookie-policy :none}) :body))]
		  (map 
		  (fn [n] (vector n "en")) 
		  (distinct 
			(flatten 
			  (map 
				(fn [n] (clojure.string/lower-case (get n "title"))) 
				(get (nth (first (get (get jres "query") "pages")) 1) "links")))))
		)
	))

(defn -main
  "main function."
  [& args]
  (do 
	(def articles-to-fetch (list
		[ "Fruit" "en" ]
		[ "Vegetable" "en" ]
		[ "Life" "en" ]
		[ "Earth" "en" ]
		;[ "WCNP" "en" ]
		;[ "My_Shanty,_Lake_George" "en" ]
	))
	(def wikiBow (mapv wiki-url-to-bow articles-to-fetch) ) ; list of bag of words for topic model
	
	;(def da-links (distinct (reduce concat (map  wiki-url-to-links-list  articles-to-fetch))))
	
	;(pprint/pprint  da-links)
	
	;(def da-links-2 (distinct (reduce concat (map  wiki-url-to-links-list  da-links))))

	;(def documentsBows [ (wcount (clojure.string/split "this vector of fruit fruit fruit fruit apples looks like an orange football" #" ")) ] )
	(def documentsBows [ (nth wikiBow 2) (nth wikiBow 3) ] )
	
	;(pprint/pprint vocabulary)
	(def testBows [ 
		(nth wikiBow 0) ; fruit
		(nth wikiBow 1) ; vegetable
		;(wcount (clojure.string/split "fruit apples" #" "))
		;(wcount (clojure.string/split "sports baseball soccer football" #" "))
	])
	(def testModels (mapv wcount-normalize testBows))
	;(pprint/pprint testModels)
	(def bgModel {"the" 0.25 "this" 0.25 "of" 0.25 "or" 0.25} ) ; 100% chance of generating "the"
	(def bgModelProb 0.4)
	(def vocabulary (distinct (flatten [(map keys documentsBows) (keys bgModel) (flatten (map keys testBows))] )))
	(print "vocabulary: ")
	(pprint/pprint vocabulary)

	(pprint/pprint (topic-probs documentsBows vocabulary testModels bgModel bgModelProb))
	
	
	;(def many-wcount-maps (map  wiki-url-to-bow  urls-to-fetch))
	;(def blah (wcount-merge many-wcount-maps))
	;(pprint/pprint blah)
	
	;(def html-root (nth text 1))
	
	; (def text (with-open [rdr (clojure.java.io/reader (first args))]
	;	(clojure.string/join "\n" (line-seq rdr))))
	
	;(def anchors-in-body (html/select text [:body :a]) )
	;(def divs-with-juicer-class (html/select text [:body :div.juicer ]) )
	
	;(def select-word-header-cambridge (html/select text [:div.cdo-section-title-hw]))
	
	; (def words (filterv (fn [x] (not (= x ""))) (mapv remove-punc (clojure.string/split (clojure.string/lower-case text) #" "))))
		
	; Now with a word-count-dict, evaluate what topic it fits.
	; getting data from wikipedia https://github.com/dakrone/clj-http
	; hopefully do this to mine topics? 
	; might also be done for getting dict.cc data. 
	; https://github.com/cgrand/enlive for html parsing.
	
))
