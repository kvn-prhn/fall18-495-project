(ns process-text.core
  (:gen-class)
  (:require [net.cgrand.enlive-html :as html])
  (:require [org.httpkit.client :as httpkit])
  (:require [clojure.pprint :as pprint])
  (:require [clj-http.client :as client])
  (:require [cheshire.core :as cheshire])
  )

(defn fetch-url [url]
	(html/html-resource (java.net.URL. url)))  ;TODO have a case for handling exceptions.

; NOT DONE YET
(defn load-word-list [f]
	"Load a file and return all of the words as a sequence."
	(with-open [rdr (clojure.java.io/reader f)]
		(clojure.string/join "\n" (distinct (line-seq rdr)))  
	))

(defn remove-punc 
	"Remove punctuation from the given string."
	[x] 
	(clojure.string/trim-newline 
		(clojure.string/replace
			(clojure.string/replace 
				(clojure.string/trim-newline x) 
				#"[\"\.\\,=+%()\$!?<>;']" 
				" ")
			"\n" ; second replace
			"")))

(defn extract-wiki-text 
	"given a wikipedia article in enlive form, get only the article text."
	[text]
	(defn iter-down
		[t] 
		(if (seq? t)
			(concat (map iter-down t))
			(if (nil? (get t :content))
				t
				(iter-down (get t :content))
			)
		)
	)
	(filter (fn [n] 
				(and
				(not (clojure.string/blank? n))
				(not (nil? (re-matches #"[0-9a-zA-Z]*" n)))
				(not (nil? (re-matches #"[a-zA-Z]*" n)))
			))
		(map clojure.string/lower-case (map clojure.string/trim (clojure.string/split 
			(remove-punc (clojure.string/join " " (flatten (iter-down 
				(flatten (html/select text [:div.mw-body-content :div.mw-parser-output :p])) 
			)))) ; this is the article
			#" "
	)))))
	
	
(defn wcount 
	"count the number of occurances of each word. Returns a dictionary."
	[wlist]
	(let [mcount_input {}]
	  (loop [i 0 mcount mcount_input]
		(if (>= i (count wlist))
		  mcount
		  (recur (inc i) (if (nil? (get mcount (nth wlist i)))
			(assoc mcount (nth wlist i) 1)
			(update mcount (nth wlist i) inc)
		  ))
		)
	))
)

(defn wcount-merge
	"given a list of word count maps, merge them all together and sum counts"
	[wlists]
	(println wlists)
	(reduce 
		(fn [wlist1 wlist2] 
			(loop [result wlist1 q wlist2]
				(if (= 0 (count q))
					result
					(let [ nextkey (first (keys q)) rmq (dissoc q nextkey)]
						(if (nil? (get result nextkey))
							(recur (assoc result nextkey 
								(get q nextkey)) rmq )
							(recur (assoc result nextkey 
								(+ (get result nextkey) (get q nextkey))) rmq )
						)
					)
				)
			)
		)
	wlists)
)

(defn wcount-normalize
	"given a word count map, return the same word count map but normalized."
	[wcount]
	(let [words (keys wcount) superCountSum 
		(loop [k 0 resultSum 0]
		  (if (>= k (count wcount))
		    resultSum
			(recur (inc k) (+ resultSum 
			  (get wcount (nth words k))
			))
		  )
		)
	  ]
	  (loop [k 0 resD {}]
	    (if (>= k (count wcount))
		  resD
		  (recur (inc k) (assoc resD (nth (keys wcount) k)
		    (/ (get wcount (nth (keys wcount) k)) superCountSum)
		  ))
		)
	  ) ; TODO
	)
)

(defn word-in-model 
	"get the chance that a word was generated using the given model m."
	[m w] ; model, word
	(if (contains? m w)
		(get m w)
		0))

(defn count-in-doc
	"get the count of a word w from a document d."
	[d w] ; model, word
	(if (contains? d w)
		(get d w)
		0))

(defn topic-probs
	"Given a collection of documents BOWs, a background model, the background probability, 
	and a list of topic models, determine probability of each topic being generated 
	by each topic model. 
	If the topic models are [t0 t1 ... tm] this returns a probability distribution 
	in the form [p0 p1 ... pm] where each number corresponds to each topic model."
	[ documentsBows ; a vector of ALL documents as Bags-of-Words.   (map from string to word count.)
	  vocabulary ; a list of ALL words in all the documents.
	  bgModel ; background model (map from string to probability) 
	  bgProp ; probability of the background model being used. 0 <= bgProp <= 1
	  topicModels ; vector of topic models - NOTE; THIS IS NOT BEING USED RIGHT NOW PUT IT IN THE RIGHT SPOT !!
		; https://www.youtube.com/watch?v=hrSjJo1Z-UE&index=27&list=PLLssT5z_DsK8Xwnh_0bjN4KNT81bekvtt
	]
	(let [ ; these don't change inside of the loop below.
	  totalWordCounts (wcount-merge documentsBows)
	  topicIndices (loop [i 0 t [-1] ] ; bg model = -1
		  (if (>= i (count topicModels))
			t
			(recur (inc i) (conj t i)) 
		  )
		)
	  documentIndices (loop [i 0 d [] ] ; bg model = -1
		  (if (>= i (count documentsBows))
			d
			(recur (inc i) (conj d i)) 
		  )
		)
	]
	(loop [ ; ITERATING ON n
	  ; probabilites of each document being generated by the given 
	  ; topic model. This can be viewed as a matrix of documents x topic index = probability.
	  ; this is the pi_(d,j) part, where d = document and j = topic. 
	  ; note that pi_(d,bgModel) is not included. 
	  docGenByTopicProb (loop [i 0 d {}] ; make a dictionary entry for each document
			(if (>= i (count documentIndices))
		      d 
			  (recur (inc i) (assoc d (nth documentIndices i) 
				(loop [j 0 tp {}] ; make a dictionary associating each topic to a probability.
				  (if (>= j (count topicModels))
					tp 
					(recur (inc j) (assoc tp j (/ 1 (count topicModels)))) ; assume uniform probabilty.
				  )
				)
			  ))
		  ))
	  ; the probabilies of each word being generated by each topic
	  ; can be thought of as a matrix of topic index x word = probability. 
	  ; this is the P(w|tj), where w = word and tj = topic model.
	  ; this is initialzed randomly. Note that the background model is NOT in this list. 
	  wordGenByTopicProb (loop [t 0 dt {}]
		(if (>= t (count topicModels))
		  dt
		  (recur (inc t) (assoc dt t (loop [w 0 dw {}]
			(if (>= w (count vocabulary))
				dw
				(recur (inc w) (assoc dw (nth vocabulary w) (/ 1 (count vocabulary))))
			))))
		)
	  )
		n 0; which iteration the system is on
	  ] ; end let for recurinf=g
	    (println n)
		(if (= n 4) ; ending condition
			docGenByTopicProb ; return the probability of each topic generating each document.
			; E-Step; update hidden variables
			(let [				
			  ; hidden z variables for each document and word. Equal to 
			  ; the index topic model that likely genered that word in that document.
			  ; if this value is -1, then it was most likely generated by the bg model.
			  ; this can be viewed as a matrix of document x word x topic index = probability 
			  ; this is the P(z_(d,w) = j) part where d = document, w = word, and j = topic
			  ; note that P(z_(d,bgModel) = j) NOT included..
			  zwordInDocByTopicProb (loop [i 0 d {}] ; make a dictionary entry for each document
				(if (>= i (count documentIndices))
				  d 
				  (recur (inc i) (assoc d (nth documentIndices i) 
					(loop [wIndex 0 wordsd {}] 
					  (let [wordsInDoc (keys (nth documentsBows i))]; make a dictionary associating each topic to a probability.
					  (if (>= wIndex (count wordsInDoc))
						wordsd 
						(let [currWord (nth wordsInDoc wIndex)
							  sumProbOfWordFromDocument ; sum of all topic model probs. for this word in this document.
								  (loop [k 0 resultSum 0]
									(if (>= k (count topicModels))
									  resultSum
									  (recur (inc k)
										(+ resultSum 
										  (*
											(get (get docGenByTopicProb i) k) ; chance of document i being generated by topic k
											(get (get wordGenByTopicProb k) currWord)
										  )
										)
									  )
									)
								  ) ; end sumProbOfWordFromDocument
							 ]
						  (recur (inc wIndex) (assoc wordsd currWord
							(loop [j 0 tp {}] ; make a dictionary associating each topic to a probability.
							  (if (>= j (count topicModels))
								tp 
								(recur (inc j) (assoc tp j 
									; find the probability of topic j generating word currWord in document d.
									(if (> sumProbOfWordFromDocument 0)
										(/
										  (* 
											(get (get docGenByTopicProb i) j) ; chance that document i was generated using topic model j
											(word-in-model (get wordGenByTopicProb j) currWord))
										  sumProbOfWordFromDocument
										)
										0)
									;(/ 1 (count topicModels)) ; initial probability of the hidden variable.
								)) 
							  )
						  )))
						)
					  ))
					)
				  ))
				)) ; end zwordInDocByTopicProb
						
				; chance of each word in each document being generated by the background model.
				; this is also part of the E-step.
				zWordInDocBgModelProb (loop [i 0 d {}] ; make a dictionary entry for each document
					(if (>= i (count documentIndices))
					  d 
					  (recur (inc i) (assoc d (nth documentIndices i) 
						(loop [wIndex 0 wordsd {}] 
						  (let [wordsInDoc (keys (nth documentsBows i))]; make a dictionary associating each topic to a probability.
						  (if (>= wIndex (count wordsInDoc))
							wordsd 
							(let [currWord (nth wordsInDoc wIndex)]
								(recur (inc wIndex) (assoc wordsd currWord
								  (/
									(* bgProp (word-in-model  bgModel currWord))
									(+ (* bgProp (word-in-model  bgModel currWord)) 
									  (* (- 1 bgProp) 
										(loop [k 0 resultSum 0]
										  (if (>= k (count topicModels)) ; kth topic model
											resultSum
											(recur (inc k) 
												(+ resultSum 
												(*  
												  (get (get docGenByTopicProb i) k) ; probability this document was generated by the kth topic
												  (word-in-model (get wordGenByTopicProb k) currWord) ; the probability this word was generated by the kth topic
												)))
										  )
										)
									  )
									)
								  )
							  )))
						  ))
						)
					  ))
				  )) ; end finding zWordInDocBgModelProb
			  ]
			  (do 
			    (pprint/pprint docGenByTopicProb)
			    ;(pprint/pprint wordGenByTopicProb)
			    ;(pprint/pprint zWordInDocBgModelProb)
			    ;(pprint/pprint zwordInDocByTopicProb)
			    (recur 
				  ; M-step; update the words by topic and document by topic values.
				  ;docGenByTopicProb
				  (loop [i 0 d {}] ; make a dictionary entry for each document
					(if (>= i (count documentIndices))
					  d 
					  (let [currDocBow (nth documentsBows i)] 
					   (recur (inc i) (assoc d (nth documentIndices i) 
						(let [
						  sumProbOfDocumentsAndWords ; each topic iteration shares this value.
						  (loop [topicJ 0 resultSum 0] ; iterate over topic models
						    (if (>= topicJ (count topicModels))
							  resultSum
							  (recur (inc topicJ) (+ resultSum
							    (loop [wIndex 0 innerResultSum 0] ; terate inside over the vocabulary
								  (if (>= wIndex (count vocabulary))
								    innerResultSum
									(recur (inc wIndex) 
									  (let [currWordP (nth vocabulary wIndex)]
									    (+ innerResultSum
										  (*
											(count-in-doc currDocBow currWordP)
											(- 1 (get (get zWordInDocBgModelProb i) currWordP)) ; 1 - chance the current word in document i was generated by background prob.
											(get (get (get zwordInDocByTopicProb i) currWordP) topicJ) ; chance the current word in document i was generated with topic model j
										  )
									  ))
									)))
								))
							)
						  ) ; end sumProbOfDocumentsAndWords
						]
						; iterating over all of the topics
						(loop [j 0 tp {}]  
						  (if (>= j (count topicModels))
							tp 
							(recur (inc j) (assoc tp j 
							  (/
							    (loop [w1 0 resultSum 0]
								  (if (>= w1 (count vocabulary))
								    resultSum
									(recur (inc w1) 
									  (let [currWord (nth vocabulary w1)]
									    (+ resultSum
										  (*
											(count-in-doc currDocBow currWord)
											(- 1 (get (get zWordInDocBgModelProb i) currWord)) ; 1 - chance the current word in document i was generated by background prob.
											(get (get (get zwordInDocByTopicProb i) currWord) j) ; chance the current word in document i was generated with topic model j
										  )
									  ))
									))) ; end numerator sum
								sumProbOfDocumentsAndWords ; since each topic shares this. 
							  )
							  ;(/ 1 (count topicModels))
							)) ; assume uniform probabilty.
						  )
						))
					  )))
				  )) ; end docGenByTopicProb
				  ;wordGenByTopicProb
				  (loop [t 0 dt {}]
					(if (>= t (count topicModels))
					  dt
					  (let [
						  sumOfAllDocAndWordProbs ; each word shares this.
						  (loop [w 0 resultSum 0]
						    (if (>= w (count vocabulary))
							  resultSum
							  (let [currWord (nth vocabulary w)]
							  (recur (inc w) (+ resultSum
							    (loop [indexD 0 innerResultSum 0]
								  (if (>= indexD (count documentsBows))
									innerResultSum
									(recur (inc indexD) (+ innerResultSum 
									  (*
									    (count-in-doc (nth documentsBows indexD) currWord)
										(- 1 (get (get zWordInDocBgModelProb indexD) currWord))
										(get (get (get zwordInDocByTopicProb indexD) currWord) t)
									  )
									))
								  )
								)
							  )))
							)
						  ) ; end sumOfAllDocAndWordProbs
						] (recur (inc t) (assoc dt t (loop [w 0 dw {}]
						(if (>= w (count vocabulary))
							dw
							(let [currWord (nth vocabulary w)] 
							(recur (inc w) (assoc dw currWord 
								(if (> sumOfAllDocAndWordProbs 0)
									(/
									  (loop [indexD 0 resultSum 0]
										(if (>= indexD (count documentsBows))
										  resultSum
										  (recur (inc indexD) (+ resultSum 
											(*
											  (count-in-doc (nth documentsBows indexD) currWord)
											  (- 1 (get (get zWordInDocBgModelProb indexD) currWord))
											  (get (get (get zwordInDocByTopicProb indexD) currWord) t)
											)
										  ))
										)
									  )
									  sumOfAllDocAndWordProbs
									)
									0)
							))
						))))))
					)) ; end wordGenByTopicProb
   				  (inc n) ; increase the iteration number.
			))) ; end second part of if statment that checks to keep recurring loop
		)
	) ; end loop
	) ; end first let
)

(defn wiki-url-to-bow 
	"Given a wikipedia title and language, convert the article to a Bag of Words with counts."
	[v] ;[title lang-short]
	(let [title (nth v 0) 
		  lang-short (nth v 1)	
		  url (clojure.string/join "" [ "https://" lang-short ".wikipedia.org/wiki/" title ])]
	(println (clojure.string/join "" ["Getting wikipedia article at " url]))
	(wcount (extract-wiki-text (fetch-url url)))))

(defn wiki-url-to-links-list
	"Given a wikipedia title and language, get the first 10 titles of articles that are links to the given article.."
	[v] ;[title lang-short]
	(let [title (nth v 0) 
	  lang-short (nth v 1)	
	  url (clojure.string/join "" [ "https://" lang-short ".wikipedia.org/w/api.php?action=query&prop=links&format=json&titles=" title ])]
		(let [jres (cheshire/parse-string (get (client/get url {:cookie-policy :none}) :body))]
		  (map 
		  (fn [n] (vector n "en")) 
		  (distinct 
			(flatten 
			  (map 
				(fn [n] (clojure.string/lower-case (get n "title"))) 
				(get (nth (first (get (get jres "query") "pages")) 1) "links")))))
		)
	))

(defn -main
  "main function."
  [& args]
  (do 
  
	(def articles-to-fetch (list
		;[ "Earth" "en" ]
		[ "Fruit" "en" ]
		;[ "Life" "en" ]
		;[ "WCNP" "en" ]
		;[ "My_Shanty,_Lake_George" "en" ]
	))
	
	;(def da-links (distinct (reduce concat (map  wiki-url-to-links-list  articles-to-fetch))))
	
	;(pprint/pprint  da-links)
	
	;(def da-links-2 (distinct (reduce concat (map  wiki-url-to-links-list  da-links))))

	(def documentsBows [ (wcount (clojure.string/split "this vector of apples looks like an orange football or soccer" #" ")) ] )
	;(def wikiBow (map wiki-url-to-bow articles-to-fetch) ) ; list of bag of words for topic model
	(def vocabulary (flatten (map keys documentsBows)) )
	;(pprint/pprint vocabulary)
	(def testBows [ 
		(wcount (clojure.string/split "fruit apples" #" "))
		(wcount (clojure.string/split "sports baseball soccer football" #" "))
	])
	(def testModels (map wcount-normalize testBows))
	(pprint/pprint testModels)
	(def bgModel {"the" 0.25 "this" 0.25 "of" 0.25 "or" 0.25} ) ; 100% chance of generating "the"
	(def bgModelProb 0.7)
	(pprint/pprint (topic-probs documentsBows vocabulary bgModel bgModelProb testModels))
	
	;(def many-wcount-maps (map  wiki-url-to-bow  urls-to-fetch))
	;(def blah (wcount-merge many-wcount-maps))
	;(pprint/pprint blah)
	
	;(def html-root (nth text 1))
	
	; (def text (with-open [rdr (clojure.java.io/reader (first args))]
	;	(clojure.string/join "\n" (line-seq rdr))))
	
	;(def anchors-in-body (html/select text [:body :a]) )
	;(def divs-with-juicer-class (html/select text [:body :div.juicer ]) )
	
	;(def select-word-header-cambridge (html/select text [:div.cdo-section-title-hw]))
	
	; (def words (filterv (fn [x] (not (= x ""))) (mapv remove-punc (clojure.string/split (clojure.string/lower-case text) #" "))))
		
	; Now with a word-count-dict, evaluate what topic it fits.
	; getting data from wikipedia https://github.com/dakrone/clj-http
	; hopefully do this to mine topics? 
	; might also be done for getting dict.cc data. 
	; https://github.com/cgrand/enlive for html parsing.
	
))
